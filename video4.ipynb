{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.master(\"local[*]\").appName(\"SparkPractice\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+\n",
      "| id|    name|age|salary|     address| nominee|\n",
      "+---+--------+---+------+------------+--------+\n",
      "|  1|  Manish| 26| 75000|       bihar|nominee1|\n",
      "|  2|  Nikita| 23|100000|uttarpradesh|nominee2|\n",
      "|  3|  Pritam| 22|150000|   Bangalore|   India|\n",
      "|  4|Prantosh| 17|200000|     Kolkata|   India|\n",
      "|  5|  Vikash| 31|300000|        NULL|nominee5|\n",
      "+---+--------+---+------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\",\"true\")\\\n",
    ".option(\"inferschema\",\"true\")\\\n",
    ".option(\"mode\",\"permissive\")\\\n",
    ".load(r\"C:\\Users\\jenil.makhansa\\dataset.csv\")\n",
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType,StructField,StructType,IntegerType\n",
    "emp_schema = StructType(\n",
    "    [\n",
    "        StructField(\"id\",IntegerType(),True),\n",
    "        StructField(\"name\",StringType(),True),\n",
    "        StructField(\"age\",IntegerType(),True),\n",
    "        StructField(\"salary\",IntegerType(),True),\n",
    "        StructField(\"address\",StringType(),True),\n",
    "        StructField(\"nominee\",StringType(),True),\n",
    "        StructField(\"_corrupt_record\",StringType(),True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+--------------------+\n",
      "| id|    name|age|salary|     address| nominee|     _corrupt_record|\n",
      "+---+--------+---+------+------------+--------+--------------------+\n",
      "|  1|  Manish| 26| 75000|       bihar|nominee1|                NULL|\n",
      "|  2|  Nikita| 23|100000|uttarpradesh|nominee2|                NULL|\n",
      "|  3|  Pritam| 22|150000|   Bangalore|   India|3,Pritam,22,15000...|\n",
      "|  4|Prantosh| 17|200000|     Kolkata|   India|4,Prantosh,17,200...|\n",
      "|  5|  Vikash| 31|300000|        NULL|nominee5|                NULL|\n",
      "+---+--------+---+------+------------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\",\"true\")\\\n",
    ".option(\"inferschema\",\"true\")\\\n",
    ".schema(emp_schema)\\\n",
    ".option(\"badRecordsPath\",r\"C:\\Users\\jenil.makhansa\\badrecords\")\\\n",
    ".load(r\"C:\\Users\\jenil.makhansa\\dataset.csv\")\n",
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n",
      "|age|    name|salary|\n",
      "+---+--------+------+\n",
      "| 20|  Manish| 20000|\n",
      "| 25|  Nikita| 21000|\n",
      "| 16|  Pritam| 22000|\n",
      "| 35|Prantosh| 25000|\n",
      "| 67|  Vikash| 40000|\n",
      "+---+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "line_delimited_json = spark.read.format(\"json\")\\\n",
    ".option(\"inferSchema\",\"true\")\\\n",
    ".option(\"mode\",\"PERMISSIVE\")\\\n",
    ".load(r\"C:\\Users\\jenil.makhansa\\line_delimited_json.json\")\n",
    "line_delimited_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(r\"C:\\Users\\jenil.makhansa\\sample-parquet.gz.parquet\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyarrow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpa\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpq\u001b[39;00m\n\u001b[32m      4\u001b[39m parquet_file = pq.ParquetFile(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mnikita\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDownloads\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mSpark-The-Definitive-Guide-master\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mflight-data\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mparquet\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m2010-summary.parquet\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mpart-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pyarrow'"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "parquet_file = pq.ParquetFile(r'C:\\Users\\nikita\\Downloads\\Spark-The-Definitive-Guide-master\\data\\flight-data\\parquet\\2010-summary.parquet\\part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet')\n",
    "parquet_file.metadata\n",
    "parquet_file.metadata.row_group(0) \n",
    "parquet_file.metadata.row_group(0).column(0)\n",
    "parquet_file.metadata.row_group(0).column(0).statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command1 : parquet-tools show path \n",
    "command2 : parquet-tools inspect path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.write.format(\"csv\")\\\n",
    ".option(\"header\",\"true\")\\\n",
    ".mode(\"errorifexists\")\\\n",
    ".option(\"path\",r\"C:\\Users\\jenil.makhansa\\Export_data\")\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- nominee: string (nullable = true)\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'name', 'age', 'salary', 'address', 'nominee', '_corrupt_record']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employee_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    name|\n",
      "+--------+\n",
      "|  Manish|\n",
      "|  Nikita|\n",
      "|  Pritam|\n",
      "|Prantosh|\n",
      "|  Vikash|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    name|\n",
      "+--------+\n",
      "|  Manish|\n",
      "|  Nikita|\n",
      "|  Pritam|\n",
      "|Prantosh|\n",
      "|  Vikash|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.select(col(\"name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `id+5` cannot be resolved. Did you mean one of the following? [`id`, `age`, `name`, `address`, `nominee`].;\n'Project ['id+5]\n+- Relation [id#542,name#543,age#544,salary#545,address#546,nominee#547,_corrupt_record#548] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43memployee_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mid+5\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\spark\\spark-3.5.5-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:3229\u001b[39m, in \u001b[36mDataFrame.select\u001b[39m\u001b[34m(self, *cols)\u001b[39m\n\u001b[32m   3184\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cols: \u001b[33m\"\u001b[39m\u001b[33mColumnOrName\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mDataFrame\u001b[39m\u001b[33m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   3185\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[32m   3186\u001b[39m \n\u001b[32m   3187\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3227\u001b[39m \u001b[33;03m    +-----+---+\u001b[39;00m\n\u001b[32m   3228\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3229\u001b[39m     jdf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3230\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\spark\\spark-3.5.5-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\spark\\spark-3.5.5-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `id+5` cannot be resolved. Did you mean one of the following? [`id`, `age`, `name`, `address`, `nominee`].;\n'Project ['id+5]\n+- Relation [id#542,name#543,age#544,salary#545,address#546,nominee#547,_corrupt_record#548] csv\n"
     ]
    }
   ],
   "source": [
    "# This will not work.\n",
    "employee_df.select(\"id+5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|(id + 5)|\n",
      "+--------+\n",
      "|       6|\n",
      "|       7|\n",
      "|       8|\n",
      "|       9|\n",
      "|      10|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.select(expr(\"id+5\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|(id + 5)|\n",
      "+--------+\n",
      "|       6|\n",
      "|       7|\n",
      "|       8|\n",
      "|       9|\n",
      "|      10|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.select(col(\"id\")+5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+------------+\n",
      "| id|    name|salary|     address|\n",
      "+---+--------+------+------------+\n",
      "|  1|  Manish| 75000|       bihar|\n",
      "|  2|  Nikita|100000|uttarpradesh|\n",
      "|  3|  Pritam|150000|   Bangalore|\n",
      "|  4|Prantosh|200000|     Kolkata|\n",
      "|  5|  Vikash|300000|        NULL|\n",
      "+---+--------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.select(\"id\",col(\"name\"),employee_df[\"salary\"],employee_df.address).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------+\n",
      "|employee_id|concat(name, address)|\n",
      "+-----------+---------------------+\n",
      "|          1|          Manishbihar|\n",
      "|          2|   Nikitauttarpradesh|\n",
      "|          3|      PritamBangalore|\n",
      "|          4|      PrantoshKolkata|\n",
      "|          5|                 NULL|\n",
      "+-----------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.select(expr(\"id as employee_id\"),expr(\"concat(name,address)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.createOrReplaceTempView(\"employee_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+--------------------+\n",
      "| id|    name|age|salary|     address| nominee|     _corrupt_record|\n",
      "+---+--------+---+------+------------+--------+--------------------+\n",
      "|  1|  Manish| 26| 75000|       bihar|nominee1|                NULL|\n",
      "|  2|  Nikita| 23|100000|uttarpradesh|nominee2|                NULL|\n",
      "|  3|  Pritam| 22|150000|   Bangalore|   India|3,Pritam,22,15000...|\n",
      "|  4|Prantosh| 17|200000|     Kolkata|   India|4,Prantosh,17,200...|\n",
      "|  5|  Vikash| 31|300000|        NULL|nominee5|                NULL|\n",
      "+---+--------+---+------+------------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from employee_table\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+---+\n",
      "|employee_id|    name|age|\n",
      "+-----------+--------+---+\n",
      "|          1|  Manish| 26|\n",
      "|          2|  Nikita| 23|\n",
      "|          3|  Pritam| 22|\n",
      "|          4|Prantosh| 17|\n",
      "|          5|  Vikash| 31|\n",
      "+-----------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.select(col(\"id\").alias(\"employee_id\"),\"name\",\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+--------+--------------------+\n",
      "| id|    name|age|salary|address| nominee|     _corrupt_record|\n",
      "+---+--------+---+------+-------+--------+--------------------+\n",
      "|  4|Prantosh| 17|200000|Kolkata|   India|4,Prantosh,17,200...|\n",
      "|  5|  Vikash| 31|300000|   NULL|nominee5|                NULL|\n",
      "+---+--------+---+------+-------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.filter(col(\"salary\")>150000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+--------+--------------------+\n",
      "| id|    name|age|salary|address| nominee|     _corrupt_record|\n",
      "+---+--------+---+------+-------+--------+--------------------+\n",
      "|  4|Prantosh| 17|200000|Kolkata|   India|4,Prantosh,17,200...|\n",
      "|  5|  Vikash| 31|300000|   NULL|nominee5|                NULL|\n",
      "+---+--------+---+------+-------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.where(col(\"salary\")>150000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+-------+--------------------+\n",
      "| id|    name|age|salary|address|nominee|     _corrupt_record|\n",
      "+---+--------+---+------+-------+-------+--------------------+\n",
      "|  4|Prantosh| 17|200000|Kolkata|  India|4,Prantosh,17,200...|\n",
      "+---+--------+---+------+-------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.filter((col(\"salary\")>150000) & (col(\"age\")<18)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+--------------------+---------+\n",
      "| id|    name|age|salary|     address| nominee|     _corrupt_record|last_name|\n",
      "+---+--------+---+------+------------+--------+--------------------+---------+\n",
      "|  1|  Manish| 26| 75000|       bihar|nominee1|                NULL|    kumar|\n",
      "|  2|  Nikita| 23|100000|uttarpradesh|nominee2|                NULL|    kumar|\n",
      "|  3|  Pritam| 22|150000|   Bangalore|   India|3,Pritam,22,15000...|    kumar|\n",
      "|  4|Prantosh| 17|200000|     Kolkata|   India|4,Prantosh,17,200...|    kumar|\n",
      "|  5|  Vikash| 31|300000|        NULL|nominee5|                NULL|    kumar|\n",
      "+---+--------+---+------+------------+--------+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.select(\"*\",lit(\"kumar\").alias(\"last_name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+--------------------+-------+\n",
      "| id|    name|age|salary|     address| nominee|     _corrupt_record|surname|\n",
      "+---+--------+---+------+------------+--------+--------------------+-------+\n",
      "|  1|  Manish| 26| 75000|       bihar|nominee1|                NULL|  singh|\n",
      "|  2|  Nikita| 23|100000|uttarpradesh|nominee2|                NULL|  singh|\n",
      "|  3|  Pritam| 22|150000|   Bangalore|   India|3,Pritam,22,15000...|  singh|\n",
      "|  4|Prantosh| 17|200000|     Kolkata|   India|4,Prantosh,17,200...|  singh|\n",
      "|  5|  Vikash| 31|300000|        NULL|nominee5|                NULL|  singh|\n",
      "+---+--------+---+------+------------+--------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.withColumn(\"surname\",lit(\"singh\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+---+------+------------+--------+--------------------+\n",
      "|employee_id|    name|age|salary|     address| nominee|     _corrupt_record|\n",
      "+-----------+--------+---+------+------------+--------+--------------------+\n",
      "|          1|  Manish| 26| 75000|       bihar|nominee1|                NULL|\n",
      "|          2|  Nikita| 23|100000|uttarpradesh|nominee2|                NULL|\n",
      "|          3|  Pritam| 22|150000|   Bangalore|   India|3,Pritam,22,15000...|\n",
      "|          4|Prantosh| 17|200000|     Kolkata|   India|4,Prantosh,17,200...|\n",
      "|          5|  Vikash| 31|300000|        NULL|nominee5|                NULL|\n",
      "+-----------+--------+---+------+------------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.withColumnRenamed(\"id\",\"employee_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- nominee: string (nullable = true)\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.withColumn(\"id\",col(\"id\").cast(\"string\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------------+--------+---------------+\n",
      "|age|salary|     address| nominee|_corrupt_record|\n",
      "+---+------+------------+--------+---------------+\n",
      "| 26| 75000|       bihar|nominee1|           NULL|\n",
      "| 23|100000|uttarpradesh|nominee2|           NULL|\n",
      "| 22|150000|   Bangalore|   India|           NULL|\n",
      "| 17|200000|     Kolkata|   India|           NULL|\n",
      "| 31|300000|        NULL|nominee5|           NULL|\n",
      "+---+------+------------+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.drop(\"id\",col(\"name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+-------+--------------------+\n",
      "| id|    name|age|salary|address|nominee|     _corrupt_record|\n",
      "+---+--------+---+------+-------+-------+--------------------+\n",
      "|  4|Prantosh| 17|200000|Kolkata|  India|4,Prantosh,17,200...|\n",
      "+---+--------+---+------+-------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select * from employee_table where salary > 150000 and age<18\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+-------+--------------------+---------+\n",
      "| id|    name|age|salary|address|nominee|     _corrupt_record|last_name|\n",
      "+---+--------+---+------+-------+-------+--------------------+---------+\n",
      "|  4|Prantosh| 17|200000|Kolkata|  India|4,Prantosh,17,200...|    kumar|\n",
      "+---+--------+---+------+-------+-------+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select *,\"kumar\" as last_name from employee_table where salary > 150000 and age<18\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+-------+--------------------+---------+-------------+\n",
      "| id|    name|age|salary|address|nominee|     _corrupt_record|last_name|    full_name|\n",
      "+---+--------+---+------+-------+-------+--------------------+---------+-------------+\n",
      "|  4|Prantosh| 17|200000|Kolkata|  India|4,Prantosh,17,200...|    kumar|Prantoshkumar|\n",
      "+---+--------+---+------+-------+-------+--------------------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select *,\"kumar\" as last_name,concat(name,last_name) as full_name from employee_table where salary > 150000 and age<18\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+-------+--------------------+---------+-------------+---+\n",
      "| id|    name|age|salary|address|nominee|     _corrupt_record|last_name|    full_name| id|\n",
      "+---+--------+---+------+-------+-------+--------------------+---------+-------------+---+\n",
      "|  4|Prantosh| 17|200000|Kolkata|  India|4,Prantosh,17,200...|    kumar|Prantoshkumar|  4|\n",
      "+---+--------+---+------+-------+-------+--------------------+---------+-------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select *,\"kumar\" as last_name,concat(name,last_name) as full_name,cast(id as string) from employee_table where salary > 150000 and age<18\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
